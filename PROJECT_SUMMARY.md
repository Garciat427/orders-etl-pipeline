# Related Items Pipeline - Project Summary

## ğŸ¯ Project Overview
A complete Python-based data pipeline for analyzing Shopify order data and generating product recommendations based on co-purchase patterns.

## ğŸ“ Complete File Structure

```
related-items-pipeline/
â”œâ”€â”€ ğŸ“„ README.md                    # Comprehensive setup and usage guide
â”œâ”€â”€ ğŸ“„ PROJECT_SUMMARY.md           # This file - project overview
â”œâ”€â”€ ğŸ³ docker-compose.yml           # Local dev setup (Postgres + Airflow)
â”œâ”€â”€ ğŸ“‹ requirements.txt             # Python dependencies
â”œâ”€â”€ âš™ï¸ config.py                    # Configuration settings
â”œâ”€â”€ ğŸš€ start.sh                     # Startup script (executable)
â”œâ”€â”€ ğŸ§ª test_pipeline.py             # End-to-end pipeline testing
â”‚
â”œâ”€â”€ ğŸ“Š dags/                        # Airflow DAGs
â”‚   â””â”€â”€ related_items_dag.py       # Main pipeline orchestration
â”‚
â”œâ”€â”€ ğŸ src/                         # Python scripts (ETL logic)
â”‚   â”œâ”€â”€ extract.py                  # Reads CSV and groups items by order
â”‚   â”œâ”€â”€ load.py                     # Inserts data into Postgres
â”‚   â”œâ”€â”€ transform.py                # Builds item association matrix
â”‚   â””â”€â”€ recommend.py                # Outputs JSON and writes to Postgres
â”‚
â”œâ”€â”€ ğŸ—„ï¸ db/                          # Database schema
â”‚   â””â”€â”€ init.sql                    # SQL file for schema initialization
â”‚
â””â”€â”€ ğŸ“ data/                        # Data files
    â”œâ”€â”€ orders.csv                  # Sample Shopify export file
    â””â”€â”€ related_items.json          # Generated recommendations (output)
```

## ğŸ”§ Core Components

### 1. Data Pipeline Scripts (`src/`)
- **`extract.py`**: CSV reader that groups items by order number
- **`load.py`**: Database loader using SQLAlchemy with duplicate prevention
- **`transform.py`**: Co-occurrence matrix builder with confidence calculations
- **`recommend.py`**: Results exporter to JSON and PostgreSQL

### 2. Airflow Orchestration (`dags/`)
- **`related_items_dag.py`**: 4-task pipeline with proper dependencies
- **Tasks**: extract_csv_data â†’ load_to_postgres â†’ generate_matrix â†’ export_results

### 3. Database Schema (`db/`)
- **`init.sql`**: Creates 4 tables with foreign keys and indexes
- **Tables**: orders, items, order_items, related_items
- **Features**: Cascade deletion, unique constraints, performance indexes

### 4. Infrastructure (`docker-compose.yml`)
- **PostgreSQL 13**: Database server on port 5432
- **Airflow 2.7.1**: Workflow orchestration on port 8080
- **SequentialExecutor**: Suitable for local development
- **Health checks**: Ensures services are ready before proceeding

## ğŸš€ Quick Start Commands

```bash
# 1. Start all services
cd related-items-pipeline
./start.sh

# 2. Access Airflow UI
# Open http://localhost:8080 (admin/admin)

# 3. Test individual components
python src/extract.py
python src/load.py
python src/transform.py
python src/recommend.py

# 4. Run full pipeline test
python test_pipeline.py

# 5. Stop services
docker-compose down
```

## ğŸ“Š Expected Output

### JSON Output (`data/related_items.json`)
```json
{
  "DT-MiniMaxx-V2": [
    ["DPSS-FOR-11", 0.86],
    ["EGR-FOR-11", 0.71]
  ]
}
```

### Database Query Results
```sql
SELECT * FROM related_items WHERE base_sku = 'DT-MiniMaxx-V2';
-- Returns confidence scores for related products
```

## ğŸ¯ Key Features

âœ… **Complete ETL Pipeline**: Extract â†’ Transform â†’ Load  
âœ… **Airflow Orchestration**: Automated workflow management  
âœ… **PostgreSQL Integration**: Robust data storage with constraints  
âœ… **Co-purchase Analysis**: Confidence-based recommendations  
âœ… **Docker Deployment**: Easy local development setup  
âœ… **Comprehensive Testing**: End-to-end validation  
âœ… **Production Ready**: Proper error handling and logging  
âœ… **Extensible Design**: Easy to modify and extend  

## ğŸ” Technical Details

- **Language**: Python 3.8+
- **Database**: PostgreSQL with SQLAlchemy ORM
- **Orchestration**: Apache Airflow
- **Containerization**: Docker & Docker Compose
- **Data Processing**: Pandas for CSV handling
- **Algorithm**: Co-occurrence matrix with confidence scoring
- **Output Formats**: JSON file + PostgreSQL table

## ğŸ§ª Testing & Validation

The project includes:
- **Unit testing**: Each script can run independently
- **Integration testing**: `test_pipeline.py` validates the entire flow
- **Data validation**: Checks for data integrity and completeness
- **Error handling**: Comprehensive exception handling throughout

## ğŸš€ Production Considerations

- Change default credentials
- Use environment variables for sensitive data
- Implement proper logging and monitoring
- Consider scaling with CeleryExecutor
- Add data validation and quality checks
- Implement backup and recovery procedures

## ğŸ“š Documentation

- **README.md**: Complete setup and usage guide
- **Code comments**: Inline documentation in all Python files
- **SQL comments**: Schema documentation in init.sql
- **Configuration**: Centralized settings in config.py

## ğŸ‰ Ready to Use!

The pipeline is fully functional and ready for:
1. **Local development** and testing
2. **Learning** ETL pipeline concepts
3. **Production deployment** with modifications
4. **Customization** for different data sources
5. **Extension** with additional analytics

Simply run `./start.sh` and start exploring!
